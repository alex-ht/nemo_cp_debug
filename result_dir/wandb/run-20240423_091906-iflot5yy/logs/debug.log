2024-04-23 09:19:06,726 INFO    MainThread:28721 [wandb_setup.py:_flush():76] Current SDK version is 0.16.6
2024-04-23 09:19:06,726 INFO    MainThread:28721 [wandb_setup.py:_flush():76] Configure stats pid to 28721
2024-04-23 09:19:06,726 INFO    MainThread:28721 [wandb_setup.py:_flush():76] Loading settings from /home/u9824269/.config/wandb/settings
2024-04-23 09:19:06,726 INFO    MainThread:28721 [wandb_setup.py:_flush():76] Loading settings from /home/u9824269/LLM/nemo/NeMo-Aligner/wandb/settings
2024-04-23 09:19:06,726 INFO    MainThread:28721 [wandb_setup.py:_flush():76] Loading settings from environment variables: {}
2024-04-23 09:19:06,726 INFO    MainThread:28721 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2024-04-23 09:19:06,726 INFO    MainThread:28721 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'examples/nlp/gpt/train_gpt_sft.py', 'program_abspath': '/home/u9824269/LLM/nemo/NeMo-Aligner/examples/nlp/gpt/train_gpt_sft.py', 'program': '/home/u9824269/LLM/nemo/NeMo-Aligner/examples/nlp/gpt/train_gpt_sft.py'}
2024-04-23 09:19:06,726 INFO    MainThread:28721 [wandb_setup.py:_flush():76] Applying login settings: {}
2024-04-23 09:19:06,726 INFO    MainThread:28721 [wandb_init.py:_log_setup():521] Logging user logs to /home/u9824269/LLM/llama3/train/result_dir/wandb/run-20240423_091906-iflot5yy/logs/debug.log
2024-04-23 09:19:06,726 INFO    MainThread:28721 [wandb_init.py:_log_setup():522] Logging internal logs to /home/u9824269/LLM/llama3/train/result_dir/wandb/run-20240423_091906-iflot5yy/logs/debug-internal.log
2024-04-23 09:19:06,726 INFO    MainThread:28721 [wandb_init.py:init():561] calling init triggers
2024-04-23 09:19:06,727 INFO    MainThread:28721 [wandb_init.py:init():568] wandb.init called with sweep_config: {}
config: {}
2024-04-23 09:19:06,727 INFO    MainThread:28721 [wandb_init.py:init():611] starting backend
2024-04-23 09:19:06,727 INFO    MainThread:28721 [wandb_init.py:init():615] setting up manager
2024-04-23 09:19:06,730 INFO    MainThread:28721 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2024-04-23 09:19:06,731 INFO    MainThread:28721 [wandb_init.py:init():623] backend started and connected
2024-04-23 09:19:06,734 INFO    MainThread:28721 [wandb_init.py:init():715] updated telemetry
2024-04-23 09:19:06,788 INFO    MainThread:28721 [wandb_init.py:init():748] communicating run to backend with 90.0 second timeout
2024-04-23 09:19:07,501 INFO    MainThread:28721 [wandb_run.py:_on_init():2357] communicating current version
2024-04-23 09:19:07,853 INFO    MainThread:28721 [wandb_run.py:_on_init():2366] got version response 
2024-04-23 09:19:07,854 INFO    MainThread:28721 [wandb_init.py:init():799] starting run threads in backend
2024-04-23 09:19:08,142 INFO    MainThread:28721 [wandb_run.py:_console_start():2335] atexit reg
2024-04-23 09:19:08,142 INFO    MainThread:28721 [wandb_run.py:_redirect():2190] redirect: wrap_raw
2024-04-23 09:19:08,142 INFO    MainThread:28721 [wandb_run.py:_redirect():2255] Wrapping output streams.
2024-04-23 09:19:08,142 INFO    MainThread:28721 [wandb_run.py:_redirect():2280] Redirects installed.
2024-04-23 09:19:08,143 INFO    MainThread:28721 [wandb_init.py:init():842] run started, returning control to user process
2024-04-23 09:42:37,187 INFO    MainThread:28721 [wandb_run.py:_config_callback():1347] config_cb None None {'name': 'megatron_gpt_sft', 'trainer': {'num_nodes': 16, 'devices': -1, 'accelerator': 'gpu', 'precision': '16-mixed', 'logger': False, 'enable_checkpointing': False, 'use_distributed_sampler': False, 'max_time': None, 'max_epochs': 1, 'max_steps': -1, 'sft': {'max_epochs': 1, 'max_steps': -1, 'val_check_interval': 12, 'save_interval': 12, 'limit_val_batches': 1, 'gradient_clip_val': 1.0}}, 'exp_manager': {'explicit_log_dir': '/home/u9824269/LLM/llama3/train/result_dir', 'exp_dir': None, 'name': 'megatron_gpt_sft', 'create_wandb_logger': True, 'wandb_logger_kwargs': {'project': 'llama3-cp-debug', 'name': 'dolly_sft_run_tp8_no_cp'}, 'resume_if_exists': True, 'resume_ignore_no_checkpoint': True, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'monitor': 'validation_loss', 'save_top_k': 5, 'mode': 'min', 'save_nemo_on_train_end': True, 'filename': 'megatron_gpt_sft--{validation_loss:.3f}-{step}-{consumed_samples}-{epoch}', 'model_parallel_size': 8, 'save_best_model': False}}, 'model': {'seed': 1234, 'tensor_model_parallel_size': 8, 'pipeline_model_parallel_size': 4, 'restore_from_path': '/home/u9824269/LLM/llama3/train/llama3-70b-16-mixed.nemo', 'resume_from_checkpoint': None, 'save_nemo_on_validation_end': True, 'sync_batch_comm': False, 'megatron_amp_O2': True, 'encoder_seq_length': 8192, 'sequence_parallel': True, 'activations_checkpoint_granularity': 'selective', 'activations_checkpoint_method': 'uniform', 'activations_checkpoint_num_layers': None, 'activations_checkpoint_layers_per_pipeline': None, 'answer_only_loss': True, 'gradient_as_bucket_view': False, 'seq_len_interpolation_factor': None, 'use_flash_attention': None, 'hidden_dropout': 0.0, 'attention_dropout': 0.0, 'ffn_dropout': 0.0, 'peft': {'peft_scheme': 'none', 'restore_from_path': None, 'lora_tuning': {'target_modules': ['attention_qkv'], 'adapter_dim': 32, 'adapter_dropout': 0.0, 'column_init_method': 'xavier', 'row_init_method': 'zero', 'layer_selection': None, 'weight_tying': False, 'position_embedding_strategy': None}}, 'data': {'chat': False, 'chat_prompt_tokens': {'system_turn_start': '\x00', 'turn_start': '\x11', 'label_start': '\x12', 'end_of_turn': '\n', 'end_of_name': '\n'}, 'sample': False, 'num_workers': 0, 'dataloader_type': 'single', 'train_ds': {'file_path': '/home/u9824269/LLM/nemo/databricks-dolly-15k-output.jsonl', 'global_batch_size': 8, 'micro_batch_size': 1, 'shuffle': True, 'memmap_workers': None, 'max_seq_length': 8192, 'min_seq_length': 1, 'drop_last': True, 'label_key': 'output', 'add_eos': True, 'add_sep': False, 'add_bos': True, 'truncation_field': 'input', 'index_mapping_dir': None, 'prompt_template': '<|start_header_id|>user<|end_header_id|>\n\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{output}', 'hf_dataset': False, 'truncation_method': 'right'}, 'validation_ds': {'file_path': '/home/u9824269/LLM/nemo/databricks-dolly-15k-output.jsonl', 'global_batch_size': 128, 'micro_batch_size': 1, 'shuffle': False, 'memmap_workers': None, 'max_seq_length': 8192, 'min_seq_length': 1, 'drop_last': True, 'label_key': 'output', 'add_eos': True, 'add_sep': False, 'add_bos': True, 'truncation_field': 'input', 'index_mapping_dir': None, 'prompt_template': '<|start_header_id|>user<|end_header_id|>\n\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{output}', 'hf_dataset': False, 'truncation_method': 'right', 'output_original_text': True}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 5e-06, 'weight_decay': 0.01, 'betas': [0.9, 0.98], 'sched': {'name': 'CosineAnnealing', 'warmup_steps': 10, 'constant_steps': 1000, 'min_lr': 9e-07}}, 'context_parallel_size': 1, 'precision': '16-mixed'}}
