
[NeMo I 2024-04-23 09:19:08 exp_manager:857] WandBLogger has been set up
[NeMo I 2024-04-23 09:27:03 megatron_init:251] Rank 0 has data parallel group : [0, 8, 16, 24]
[NeMo I 2024-04-23 09:27:03 megatron_init:257] Rank 0 has combined group of data parallel and context parallel : [0, 8, 16, 24]
[NeMo I 2024-04-23 09:27:03 megatron_init:262] All data parallel group ranks with context parallel combined: [[0, 8, 16, 24], [1, 9, 17, 25], [2, 10, 18, 26], [3, 11, 19, 27], [4, 12, 20, 28], [5, 13, 21, 29], [6, 14, 22, 30], [7, 15, 23, 31], [32, 40, 48, 56], [33, 41, 49, 57], [34, 42, 50, 58], [35, 43, 51, 59], [36, 44, 52, 60], [37, 45, 53, 61], [38, 46, 54, 62], [39, 47, 55, 63], [64, 72, 80, 88], [65, 73, 81, 89], [66, 74, 82, 90], [67, 75, 83, 91], [68, 76, 84, 92], [69, 77, 85, 93], [70, 78, 86, 94], [71, 79, 87, 95], [96, 104, 112, 120], [97, 105, 113, 121], [98, 106, 114, 122], [99, 107, 115, 123], [100, 108, 116, 124], [101, 109, 117, 125], [102, 110, 118, 126], [103, 111, 119, 127]]
[NeMo I 2024-04-23 09:27:03 megatron_init:265] Ranks 0 has data parallel rank: 0
[NeMo I 2024-04-23 09:27:03 megatron_init:282] Rank 0 has context parallel group: [0]
[NeMo I 2024-04-23 09:27:03 megatron_init:285] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
[NeMo I 2024-04-23 09:27:03 megatron_init:286] Ranks 0 has context parallel rank: 0
[NeMo I 2024-04-23 09:27:03 megatron_init:297] Rank 0 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7, 32, 33, 34, 35, 36, 37, 38, 39, 64, 65, 66, 67, 68, 69, 70, 71, 96, 97, 98, 99, 100, 101, 102, 103]
[NeMo I 2024-04-23 09:27:03 megatron_init:298] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7, 32, 33, 34, 35, 36, 37, 38, 39, 64, 65, 66, 67, 68, 69, 70, 71, 96, 97, 98, 99, 100, 101, 102, 103], [8, 9, 10, 11, 12, 13, 14, 15, 40, 41, 42, 43, 44, 45, 46, 47, 72, 73, 74, 75, 76, 77, 78, 79, 104, 105, 106, 107, 108, 109, 110, 111], [16, 17, 18, 19, 20, 21, 22, 23, 48, 49, 50, 51, 52, 53, 54, 55, 80, 81, 82, 83, 84, 85, 86, 87, 112, 113, 114, 115, 116, 117, 118, 119], [24, 25, 26, 27, 28, 29, 30, 31, 56, 57, 58, 59, 60, 61, 62, 63, 88, 89, 90, 91, 92, 93, 94, 95, 120, 121, 122, 123, 124, 125, 126, 127]]
[NeMo I 2024-04-23 09:27:03 megatron_init:308] Rank 0 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2024-04-23 09:27:03 megatron_init:312] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31], [32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47], [48, 49, 50, 51, 52, 53, 54, 55], [56, 57, 58, 59, 60, 61, 62, 63], [64, 65, 66, 67, 68, 69, 70, 71], [72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87], [88, 89, 90, 91, 92, 93, 94, 95], [96, 97, 98, 99, 100, 101, 102, 103], [104, 105, 106, 107, 108, 109, 110, 111], [112, 113, 114, 115, 116, 117, 118, 119], [120, 121, 122, 123, 124, 125, 126, 127]]
[NeMo I 2024-04-23 09:27:03 megatron_init:313] Rank 0 has tensor model parallel rank: 0
[NeMo I 2024-04-23 09:27:03 megatron_init:342] Rank 0 has pipeline model parallel group: [0, 32, 64, 96]
[NeMo I 2024-04-23 09:27:03 megatron_init:354] Rank 0 has embedding group: [0, 96]
[NeMo I 2024-04-23 09:27:03 megatron_init:360] All pipeline model parallel group ranks: [[0, 32, 64, 96], [1, 33, 65, 97], [2, 34, 66, 98], [3, 35, 67, 99], [4, 36, 68, 100], [5, 37, 69, 101], [6, 38, 70, 102], [7, 39, 71, 103], [8, 40, 72, 104], [9, 41, 73, 105], [10, 42, 74, 106], [11, 43, 75, 107], [12, 44, 76, 108], [13, 45, 77, 109], [14, 46, 78, 110], [15, 47, 79, 111], [16, 48, 80, 112], [17, 49, 81, 113], [18, 50, 82, 114], [19, 51, 83, 115], [20, 52, 84, 116], [21, 53, 85, 117], [22, 54, 86, 118], [23, 55, 87, 119], [24, 56, 88, 120], [25, 57, 89, 121], [26, 58, 90, 122], [27, 59, 91, 123], [28, 60, 92, 124], [29, 61, 93, 125], [30, 62, 94, 126], [31, 63, 95, 127]]
[NeMo I 2024-04-23 09:27:03 megatron_init:361] Rank 0 has pipeline model parallel rank 0
[NeMo I 2024-04-23 09:27:03 megatron_init:362] All embedding group ranks: [[0, 32, 64, 96], [1, 33, 65, 97], [2, 34, 66, 98], [3, 35, 67, 99], [4, 36, 68, 100], [5, 37, 69, 101], [6, 38, 70, 102], [7, 39, 71, 103], [8, 40, 72, 104], [9, 41, 73, 105], [10, 42, 74, 106], [11, 43, 75, 107], [12, 44, 76, 108], [13, 45, 77, 109], [14, 46, 78, 110], [15, 47, 79, 111], [16, 48, 80, 112], [17, 49, 81, 113], [18, 50, 82, 114], [19, 51, 83, 115], [20, 52, 84, 116], [21, 53, 85, 117], [22, 54, 86, 118], [23, 55, 87, 119], [24, 56, 88, 120], [25, 57, 89, 121], [26, 58, 90, 122], [27, 59, 91, 123], [28, 60, 92, 124], [29, 61, 93, 125], [30, 62, 94, 126], [31, 63, 95, 127]]
[NeMo I 2024-04-23 09:27:03 megatron_init:363] Rank 0 has embedding rank: 0
[NeMo I 2024-04-23 09:27:03 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /work/u4005115/models/llama/Meta-Llama-3-70B-Instruct/
[NeMo I 2024-04-23 09:27:04 megatron_base_model:544] Padded vocab_size: 129024, original vocab_size: 128256, dummy tokens: 768.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
24-04-23 09:27:03 - PID:28721 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 2
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:03 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: add_qkv_bias in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: rotary_interleaved in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:27:04 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/128
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 128 processes
----------------------------------------------------------------------------------------------------
Loading distributed checkpoint with TensorStoreLoadShardedStrategy
[NeMo I 2024-04-23 09:42:22 nlp_overrides:1110] Model GPTSFTModel was successfully restored from /home/u9824269/LLM/llama3/train/llama3-70b-16-mixed.nemo.
[NeMo I 2024-04-23 09:42:22 train_script_utils:169] Running full finetuning since no peft scheme is given.
      | Name  | Type          | Params
    ----------------------------------------
    0 | model | Float16Module | 2.3 B
    ----------------------------------------
    2.3 B     Trainable params
    0         Non-trainable params
    2.3 B     Total params
    9,086.173 Total estimated model params size (MB)
[NeMo I 2024-04-23 09:42:22 text_memmap_dataset:116] Building data files
[NeMo I 2024-04-23 09:42:22 text_memmap_dataset:525] Processing 1 data files using 18 workers
[NeMo I 2024-04-23 09:42:23 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.707857
[NeMo I 2024-04-23 09:42:24 text_memmap_dataset:525] Processing 1 data files using 18 workers
[NeMo I 2024-04-23 09:42:24 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.707153
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:158] Loading data files
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:249] Loading /home/u9824269/LLM/nemo/databricks-dolly-15k-output.jsonl
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.032507
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:165] Computing global indices
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:116] Building data files
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:525] Processing 1 data files using 18 workers
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.692256
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:525] Processing 1 data files using 18 workers
[NeMo I 2024-04-23 09:42:26 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.634395
[NeMo I 2024-04-23 09:42:26 text_memmap_dataset:158] Loading data files
[NeMo I 2024-04-23 09:42:26 text_memmap_dataset:249] Loading /home/u9824269/LLM/nemo/databricks-dolly-15k-output.jsonl
[NeMo I 2024-04-23 09:42:26 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.002066
[NeMo I 2024-04-23 09:42:26 text_memmap_dataset:165] Computing global indices
[NeMo I 2024-04-23 09:42:26 builders:328] Building dataloader with consumed samples: 0
[NeMo I 2024-04-23 09:42:26 builders:328] Building dataloader with consumed samples: 0
[NeMo W 2024-04-23 09:42:26 experimental:26] `<class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'>` is experimental and not ready for production yet. Use at your own risk.
[NeMo I 2024-04-23 09:42:35 megatron_gpt_model:1306] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 2.27e+09. Total number of model parameters: 7.06e+10.
[NeMo I 2024-04-23 09:42:36 modelPT:723] Optimizer config = MegatronDistributedFusedAdam (
    Parameter Group 0
        betas: [0.9, 0.98]
        bias_correction: True
        eps: 1e-08
        lr: 5e-06
        weight_decay: 0.01
    Parameter Group 1
        betas: [0.9, 0.98]
        bias_correction: True
        eps: 1e-08
        lr: 5e-06
        weight_decay: 0.0
    )
[NeMo I 2024-04-23 09:42:36 lr_scheduler:915] Scheduler "<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x2b1c130bf3d0>"
    will be used during training (effective maximum steps = 1876) -
    Parameters :
    (warmup_steps: 10
    constant_steps: 1000
    min_lr: 9.0e-07
    max_steps: 1876
    )
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  | Name  | Type          | Params
----------------------------------------
0 | model | Float16Module | 2.3 B
----------------------------------------
2.3 B     Trainable params
0         Non-trainable params
2.3 B     Total params
9,086.173 Total estimated model params size (MB)
Training steps:   0%|          | 0/1876 [00:00<?, ?it/s][NeMo W 2024-04-23 09:44:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:2848: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
      warnings.warn(










Training steps:   1%|          | 11/1876 [08:50<21:41:46, 41.88s/it, train_grad_norm=nan, train_lr=5e-6, train_loss=1.75, train_consumed_samples=88, train_step_time=40.3, train_epoch=1]

      warnings.warn("This function is only for unittest")
Validation steps: 100%|██████████| 1/1 [01:21<00:00, 81.95s/it, val_loss=1.64, val_validation_step_time=68]
Training steps:   1%|          | 11/1876 [10:52<21:41:46, 41.88s/it, train_grad_norm=nan, train_lr=5e-6, train_loss=1.92, train_consumed_samples=96, train_step_time=39.6, train_epoch=1, val_loss=1.64, val_validation_step_time=68][NeMo W 2024-04-23 09:53:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:359: UserWarning: `ModelCheckpoint(monitor='validation_loss')` could not find the monitored key in the returned metrics: ['step', 'consumed_samples', 'epoch', 'train_grad_norm', 'train_lr', 'train_loss', 'train_consumed_samples', 'train_step_time', 'train_epoch', 'val_loss', 'val_validation_step_time']. HINT: Did you call `log('validation_loss', value)` in the `LightningModule`?
      warning_cache.warn(m)
Epoch 0, global step 12: 'validation_loss' was not in top 5
Training steps:   1%|          | 11/1876 [12:29<35:18:41, 68.16s/it, train_grad_norm=nan, train_lr=5e-6, train_loss=1.92, train_consumed_samples=96, train_step_time=39.6, train_epoch=1, val_loss=1.64, val_validation_step_time=68]
Error executing job with overrides: ['trainer.precision=16-mixed', 'trainer.num_nodes=16', 'trainer.devices=-1', 'trainer.sft.limit_val_batches=1', 'trainer.sft.val_check_interval=12', 'trainer.sft.save_interval=12', 'model.megatron_amp_O2=True', 'model.restore_from_path=/home/u9824269/LLM/llama3/train/llama3-70b-16-mixed.nemo', 'model.optim.lr=5e-6', 'model.optim.name=distributed_fused_adam', 'model.answer_only_loss=True', 'model.data.num_workers=0', 'model.data.train_ds.micro_batch_size=1', 'model.data.train_ds.global_batch_size=8', 'model.data.train_ds.file_path=/home/u9824269/LLM/nemo/databricks-dolly-15k-output.jsonl', 'model.data.train_ds.max_seq_length=8192', 'model.data.train_ds.add_eos=True', 'model.data.train_ds.add_bos=True', 'model.data.validation_ds.max_seq_length=8192', 'model.data.validation_ds.micro_batch_size=1', 'model.data.validation_ds.global_batch_size=128', 'model.data.validation_ds.file_path=/home/u9824269/LLM/nemo/databricks-dolly-15k-output.jsonl', 'model.data.validation_ds.add_bos=True', 'model.data.validation_ds.add_eos=True', 'model.tensor_model_parallel_size=8', 'model.pipeline_model_parallel_size=4', 'model.sequence_parallel=True', 'model.activations_checkpoint_granularity=selective', 'model.activations_checkpoint_method=uniform', '+model.context_parallel_size=1', 'exp_manager.create_wandb_logger=True', 'exp_manager.explicit_log_dir=/home/u9824269/LLM/llama3/train/result_dir', 'exp_manager.wandb_logger_kwargs.project=llama3-cp-debug', 'exp_manager.wandb_logger_kwargs.name=dolly_sft_run_tp8_no_cp', 'exp_manager.resume_if_exists=True', 'exp_manager.resume_ignore_no_checkpoint=True', 'exp_manager.create_checkpoint_callback=True', 'exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True', 'exp_manager.checkpoint_callback_params.monitor=validation_loss']
Traceback (most recent call last):
  File "/home/u9824269/LLM/nemo/NeMo-Aligner/examples/nlp/gpt/train_gpt_sft.py", line 244, in <module>
    main()
  File "/opt/NeMo/nemo/core/config/hydra_runner.py", line 129, in wrapper
    _run_hydra(
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/u9824269/LLM/nemo/NeMo-Aligner/examples/nlp/gpt/train_gpt_sft.py", line 240, in main
    sft_trainer.fit()
  File "/home/u9824269/LLM/nemo/NeMo-Aligner/nemo_aligner/algorithms/supervised.py", line 225, in fit
    self.save(metrics, is_train_end=is_train_end)
  File "/home/u9824269/LLM/nemo/NeMo-Aligner/nemo_aligner/algorithms/supervised.py", line 245, in save
    self.ckpt_callback.custom_save(monitor_candidates=monitor_candidates, is_train_end=is_train_end)
  File "/home/u9824269/LLM/nemo/NeMo-Aligner/nemo_aligner/utils/utils.py", line 62, in custom_save_ckpt_func
    super(NeMoModelCheckpoint, self)._save_last_checkpoint(trainer, monitor_candidates)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 654, in _save_last_checkpoint
    self._save_checkpoint(trainer, filepath)
  File "/opt/NeMo/nemo/utils/callbacks/nemo_model_checkpoint.py", line 273, in _save_checkpoint
    super()._save_checkpoint(trainer, filepath)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 365, in _save_checkpoint
    trainer.save_checkpoint(filepath, self.save_weights_only)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1316, in save_checkpoint
    self._checkpoint_connector.save_checkpoint(filepath, weights_only=weights_only, storage_options=storage_options)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 506, in save_checkpoint
    _checkpoint = self.dump_checkpoint(weights_only)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 462, in dump_checkpoint
    prec_plugin_state_dict = prec_plugin.state_dict()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/amp.py", line 123, in state_dict
    return self.scaler.state_dict()
  File "/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py", line 1308, in state_dict
    "_hysteresis_tracker": self._hysteresis_tracker,
