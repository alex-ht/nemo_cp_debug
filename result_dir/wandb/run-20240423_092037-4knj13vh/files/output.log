
[NeMo I 2024-04-23 09:20:42 exp_manager:857] WandBLogger has been set up
[NeMo I 2024-04-23 09:32:32 megatron_init:251] Rank 0 has data parallel group : [0, 16]
[NeMo I 2024-04-23 09:32:32 megatron_init:257] Rank 0 has combined group of data parallel and context parallel : [0, 8, 16, 24]
[NeMo I 2024-04-23 09:32:32 megatron_init:262] All data parallel group ranks with context parallel combined: [[0, 8, 16, 24], [1, 9, 17, 25], [2, 10, 18, 26], [3, 11, 19, 27], [4, 12, 20, 28], [5, 13, 21, 29], [6, 14, 22, 30], [7, 15, 23, 31], [32, 40, 48, 56], [33, 41, 49, 57], [34, 42, 50, 58], [35, 43, 51, 59], [36, 44, 52, 60], [37, 45, 53, 61], [38, 46, 54, 62], [39, 47, 55, 63], [64, 72, 80, 88], [65, 73, 81, 89], [66, 74, 82, 90], [67, 75, 83, 91], [68, 76, 84, 92], [69, 77, 85, 93], [70, 78, 86, 94], [71, 79, 87, 95], [96, 104, 112, 120], [97, 105, 113, 121], [98, 106, 114, 122], [99, 107, 115, 123], [100, 108, 116, 124], [101, 109, 117, 125], [102, 110, 118, 126], [103, 111, 119, 127]]
[NeMo I 2024-04-23 09:32:32 megatron_init:265] Ranks 0 has data parallel rank: 0
[NeMo I 2024-04-23 09:32:32 megatron_init:282] Rank 0 has context parallel group: [0, 8]
[NeMo I 2024-04-23 09:32:32 megatron_init:285] All context parallel group ranks: [[0, 8], [1, 9], [2, 10], [3, 11], [4, 12], [5, 13], [6, 14], [7, 15], [16, 24], [17, 25], [18, 26], [19, 27], [20, 28], [21, 29], [22, 30], [23, 31], [32, 40], [33, 41], [34, 42], [35, 43], [36, 44], [37, 45], [38, 46], [39, 47], [48, 56], [49, 57], [50, 58], [51, 59], [52, 60], [53, 61], [54, 62], [55, 63], [64, 72], [65, 73], [66, 74], [67, 75], [68, 76], [69, 77], [70, 78], [71, 79], [80, 88], [81, 89], [82, 90], [83, 91], [84, 92], [85, 93], [86, 94], [87, 95], [96, 104], [97, 105], [98, 106], [99, 107], [100, 108], [101, 109], [102, 110], [103, 111], [112, 120], [113, 121], [114, 122], [115, 123], [116, 124], [117, 125], [118, 126], [119, 127]]
[NeMo I 2024-04-23 09:32:32 megatron_init:286] Ranks 0 has context parallel rank: 0
[NeMo I 2024-04-23 09:32:32 megatron_init:297] Rank 0 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7, 32, 33, 34, 35, 36, 37, 38, 39, 64, 65, 66, 67, 68, 69, 70, 71, 96, 97, 98, 99, 100, 101, 102, 103]
[NeMo I 2024-04-23 09:32:32 megatron_init:298] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7, 32, 33, 34, 35, 36, 37, 38, 39, 64, 65, 66, 67, 68, 69, 70, 71, 96, 97, 98, 99, 100, 101, 102, 103], [8, 9, 10, 11, 12, 13, 14, 15, 40, 41, 42, 43, 44, 45, 46, 47, 72, 73, 74, 75, 76, 77, 78, 79, 104, 105, 106, 107, 108, 109, 110, 111], [16, 17, 18, 19, 20, 21, 22, 23, 48, 49, 50, 51, 52, 53, 54, 55, 80, 81, 82, 83, 84, 85, 86, 87, 112, 113, 114, 115, 116, 117, 118, 119], [24, 25, 26, 27, 28, 29, 30, 31, 56, 57, 58, 59, 60, 61, 62, 63, 88, 89, 90, 91, 92, 93, 94, 95, 120, 121, 122, 123, 124, 125, 126, 127]]
[NeMo I 2024-04-23 09:32:32 megatron_init:308] Rank 0 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2024-04-23 09:32:32 megatron_init:312] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31], [32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47], [48, 49, 50, 51, 52, 53, 54, 55], [56, 57, 58, 59, 60, 61, 62, 63], [64, 65, 66, 67, 68, 69, 70, 71], [72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87], [88, 89, 90, 91, 92, 93, 94, 95], [96, 97, 98, 99, 100, 101, 102, 103], [104, 105, 106, 107, 108, 109, 110, 111], [112, 113, 114, 115, 116, 117, 118, 119], [120, 121, 122, 123, 124, 125, 126, 127]]
[NeMo I 2024-04-23 09:32:32 megatron_init:313] Rank 0 has tensor model parallel rank: 0
[NeMo I 2024-04-23 09:32:32 megatron_init:342] Rank 0 has pipeline model parallel group: [0, 32, 64, 96]
[NeMo I 2024-04-23 09:32:32 megatron_init:354] Rank 0 has embedding group: [0, 96]
[NeMo I 2024-04-23 09:32:32 megatron_init:360] All pipeline model parallel group ranks: [[0, 32, 64, 96], [1, 33, 65, 97], [2, 34, 66, 98], [3, 35, 67, 99], [4, 36, 68, 100], [5, 37, 69, 101], [6, 38, 70, 102], [7, 39, 71, 103], [8, 40, 72, 104], [9, 41, 73, 105], [10, 42, 74, 106], [11, 43, 75, 107], [12, 44, 76, 108], [13, 45, 77, 109], [14, 46, 78, 110], [15, 47, 79, 111], [16, 48, 80, 112], [17, 49, 81, 113], [18, 50, 82, 114], [19, 51, 83, 115], [20, 52, 84, 116], [21, 53, 85, 117], [22, 54, 86, 118], [23, 55, 87, 119], [24, 56, 88, 120], [25, 57, 89, 121], [26, 58, 90, 122], [27, 59, 91, 123], [28, 60, 92, 124], [29, 61, 93, 125], [30, 62, 94, 126], [31, 63, 95, 127]]
[NeMo I 2024-04-23 09:32:32 megatron_init:361] Rank 0 has pipeline model parallel rank 0
[NeMo I 2024-04-23 09:32:32 megatron_init:362] All embedding group ranks: [[0, 32, 64, 96], [1, 33, 65, 97], [2, 34, 66, 98], [3, 35, 67, 99], [4, 36, 68, 100], [5, 37, 69, 101], [6, 38, 70, 102], [7, 39, 71, 103], [8, 40, 72, 104], [9, 41, 73, 105], [10, 42, 74, 106], [11, 43, 75, 107], [12, 44, 76, 108], [13, 45, 77, 109], [14, 46, 78, 110], [15, 47, 79, 111], [16, 48, 80, 112], [17, 49, 81, 113], [18, 50, 82, 114], [19, 51, 83, 115], [20, 52, 84, 116], [21, 53, 85, 117], [22, 54, 86, 118], [23, 55, 87, 119], [24, 56, 88, 120], [25, 57, 89, 121], [26, 58, 90, 122], [27, 59, 91, 123], [28, 60, 92, 124], [29, 61, 93, 125], [30, 62, 94, 126], [31, 63, 95, 127]]
[NeMo I 2024-04-23 09:32:32 megatron_init:363] Rank 0 has embedding rank: 0
[NeMo I 2024-04-23 09:32:32 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /work/u4005115/models/llama/Meta-Llama-3-70B-Instruct/
[NeMo I 2024-04-23 09:32:33 megatron_base_model:544] Padded vocab_size: 129024, original vocab_size: 128256, dummy tokens: 768.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
24-04-23 09:32:32 - PID:6982 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 4
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:32 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:1109] The model: GPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: add_qkv_bias in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: rotary_interleaved in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-23 09:32:33 megatron_base_model:516] The model: GPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/128
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 128 processes
----------------------------------------------------------------------------------------------------
Loading distributed checkpoint with TensorStoreLoadShardedStrategy
[NeMo I 2024-04-23 09:47:53 nlp_overrides:1110] Model GPTSFTModel was successfully restored from /home/u9824269/LLM/llama3/train/llama3-70b-16-mixed.nemo.
[NeMo I 2024-04-23 09:47:53 train_script_utils:169] Running full finetuning since no peft scheme is given.
      | Name  | Type          | Params
    ----------------------------------------
    0 | model | Float16Module | 2.3 B
    ----------------------------------------
    2.3 B     Trainable params
    0         Non-trainable params
    2.3 B     Total params
    9,086.173 Total estimated model params size (MB)
[NeMo I 2024-04-23 09:47:53 text_memmap_dataset:116] Building data files
[NeMo I 2024-04-23 09:47:53 text_memmap_dataset:525] Processing 1 data files using 18 workers
[NeMo I 2024-04-23 09:47:54 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.693806
[NeMo I 2024-04-23 09:47:56 text_memmap_dataset:525] Processing 1 data files using 18 workers
[NeMo I 2024-04-23 09:47:56 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.652818
[NeMo I 2024-04-23 09:47:57 text_memmap_dataset:158] Loading data files
[NeMo I 2024-04-23 09:47:57 text_memmap_dataset:249] Loading /home/u9824269/LLM/nemo/databricks-dolly-15k-output.jsonl
[NeMo I 2024-04-23 09:47:57 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001364
[NeMo I 2024-04-23 09:47:57 text_memmap_dataset:165] Computing global indices
[NeMo I 2024-04-23 09:47:57 text_memmap_dataset:116] Building data files
[NeMo I 2024-04-23 09:47:57 text_memmap_dataset:525] Processing 1 data files using 18 workers
[NeMo I 2024-04-23 09:47:57 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.606124
[NeMo I 2024-04-23 09:47:57 text_memmap_dataset:525] Processing 1 data files using 18 workers
[NeMo W 2024-04-23 09:47:58 experimental:26] `<class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'>` is experimental and not ready for production yet. Use at your own risk.
[NeMo I 2024-04-23 09:47:58 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.604723
[NeMo I 2024-04-23 09:47:58 text_memmap_dataset:158] Loading data files
[NeMo I 2024-04-23 09:47:58 text_memmap_dataset:249] Loading /home/u9824269/LLM/nemo/databricks-dolly-15k-output.jsonl
[NeMo I 2024-04-23 09:47:58 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001367
[NeMo I 2024-04-23 09:47:58 text_memmap_dataset:165] Computing global indices
[NeMo I 2024-04-23 09:47:58 builders:328] Building dataloader with consumed samples: 0
[NeMo I 2024-04-23 09:47:58 builders:328] Building dataloader with consumed samples: 0
[NeMo I 2024-04-23 09:48:07 megatron_gpt_model:1306] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 2.27e+09. Total number of model parameters: 7.06e+10.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[NeMo I 2024-04-23 09:48:08 modelPT:723] Optimizer config = MegatronDistributedFusedAdam (
    Parameter Group 0
        betas: [0.9, 0.98]
        bias_correction: True
        eps: 1e-08
        lr: 5e-06
        weight_decay: 0.01
    Parameter Group 1
        betas: [0.9, 0.98]
        bias_correction: True
        eps: 1e-08
        lr: 5e-06
        weight_decay: 0.0
    )
[NeMo I 2024-04-23 09:48:08 lr_scheduler:915] Scheduler "<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x2ad465cd3670>"
    will be used during training (effective maximum steps = 1876) -
    Parameters :
    (warmup_steps: 10
    constant_steps: 1000
    min_lr: 9.0e-07
    max_steps: 1876
    )
  | Name  | Type          | Params
----------------------------------------
0 | model | Float16Module | 2.3 B
----------------------------------------
2.3 B     Trainable params
0         Non-trainable params
2.3 B     Total params
9,086.173 Total estimated model params size (MB)
Training steps:   0%|          | 0/1876 [00:20<?, ?it/s]
Error executing job with overrides: ['trainer.precision=16-mixed', 'trainer.num_nodes=16', 'trainer.devices=-1', 'trainer.sft.limit_val_batches=1', 'trainer.sft.val_check_interval=12', 'trainer.sft.save_interval=12', 'model.megatron_amp_O2=True', 'model.restore_from_path=/home/u9824269/LLM/llama3/train/llama3-70b-16-mixed.nemo', 'model.optim.lr=5e-6', 'model.optim.name=distributed_fused_adam', 'model.answer_only_loss=True', 'model.data.num_workers=0', 'model.data.train_ds.micro_batch_size=1', 'model.data.train_ds.global_batch_size=8', 'model.data.train_ds.file_path=/home/u9824269/LLM/nemo/databricks-dolly-15k-output.jsonl', 'model.data.train_ds.max_seq_length=8192', 'model.data.train_ds.add_eos=True', 'model.data.train_ds.add_bos=True', 'model.data.validation_ds.max_seq_length=8192', 'model.data.validation_ds.micro_batch_size=1', 'model.data.validation_ds.global_batch_size=128', 'model.data.validation_ds.file_path=/home/u9824269/LLM/nemo/databricks-dolly-15k-output.jsonl', 'model.data.validation_ds.add_bos=True', 'model.data.validation_ds.add_eos=True', 'model.tensor_model_parallel_size=8', 'model.pipeline_model_parallel_size=4', 'model.sequence_parallel=True', 'model.activations_checkpoint_granularity=selective', 'model.activations_checkpoint_method=uniform', '+model.context_parallel_size=2', 'exp_manager.create_wandb_logger=True', 'exp_manager.explicit_log_dir=/home/u9824269/LLM/llama3/train/result_dir', 'exp_manager.wandb_logger_kwargs.project=llama3-cp-debug', 'exp_manager.wandb_logger_kwargs.name=dolly_sft_run_tp8_cp', 'exp_manager.resume_if_exists=True', 'exp_manager.resume_ignore_no_checkpoint=True', 'exp_manager.create_checkpoint_callback=True', 'exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True', 'exp_manager.checkpoint_callback_params.monitor=validation_loss']
Traceback (most recent call last):
  File "/home/u9824269/LLM/nemo/NeMo-Aligner/examples/nlp/gpt/train_gpt_sft.py", line 244, in <module>
    main()
  File "/opt/NeMo/nemo/core/config/hydra_runner.py", line 129, in wrapper
    _run_hydra(
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/u9824269/LLM/nemo/NeMo-Aligner/examples/nlp/gpt/train_gpt_sft.py", line 240, in main
    sft_trainer.fit()
  File "/home/u9824269/LLM/nemo/NeMo-Aligner/nemo_aligner/algorithms/supervised.py", line 186, in fit
    loss, metrics = self.train_single_step(batch)
  File "/home/u9824269/LLM/nemo/NeMo-Aligner/nemo_aligner/algorithms/supervised.py", line 129, in train_single_step
    loss_mean, metrics = self.model.get_loss_and_metrics(batch=batch, forward_only=False)
  File "/home/u9824269/LLM/nemo/NeMo-Aligner/nemo_aligner/models/nlp/gpt/gpt_sft_model.py", line 92, in get_loss_and_metrics
    losses_reduced = fwd_bwd_function(
  File "/opt/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 1211, in forward_backward_pipelining_without_interleaving
    output_tensor = forward_step(
  File "/opt/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 192, in forward_step
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/opt/NeMo/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py", line 1015, in fwd_output_and_loss_func
    output_tensor = model(**forward_args)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/Megatron-LM/megatron/core/transformer/module.py", line 168, in forward
    outputs = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/Megatron-LM/megatron/core/models/gpt/gpt_model.py", line 170, in forward
    hidden_states = self.decoder(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/Megatron-LM/megatron/core/transformer/transformer_block.py", line 368, in forward
    hidden_states, context = layer(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/Megatron-LM/megatron/core/transformer/transformer_layer.py", line 159, in forward
    attention_output_with_bias = self.self_attention(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/Megatron-LM/megatron/core/transformer/attention.py", line 296, in forward
    core_attn_out = self._checkpointed_attention_forward(
  File "/opt/Megatron-LM/megatron/core/transformer/attention.py", line 130, in _checkpointed_attention_forward
    hidden_states = tensor_parallel.checkpoint(
  File "/opt/Megatron-LM/megatron/core/tensor_parallel/random.py", line 266, in checkpoint
    return CheckpointFunction.apply(function, distribute_saved_activations, *args)
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py", line 553, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/opt/Megatron-LM/megatron/core/tensor_parallel/random.py", line 205, in forward
    outputs = run_function(*args)
  File "/opt/Megatron-LM/megatron/core/transformer/attention.py", line 117, in custom_forward
    output_ = self.core_attention(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/Megatron-LM/megatron/core/transformer/custom_layers/transformer_engine.py", line 462, in forward
    core_attn_out = super().forward(
  File "/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py", line 3339, in forward
    return self.mem_eff_attention(query_layer,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py", line 2551, in forward
    output = attn_forward_func_with_cp(
  File "/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py", line 1183, in attn_forward_func_with_cp
    out = AttnFuncWithCP.apply(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py", line 553, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py", line 773, in forward
    flash_attn_fwd_softmax_lse_correction(softmax_lse_[..., 1, :],
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 417, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 580, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 741, in _convert_frame
    result = inner_convert(frame, cache_entry, hooks, frame_state)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 384, in _convert_frame_assert
    return _compile(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 643, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 246, in time_wrapper
    r = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 524, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object
    transformations(instructions, code_options)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 151, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 489, in transform
    tracer.run()
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2110, in run
    super().run()
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 780, in run
    and self.step()
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 743, in step
    getattr(self, inst.opname)(inst)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 462, in wrapper
    return inner_fn(self, inst)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 1190, in CALL_FUNCTION
    self.call_function(fn, args, {})
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 644, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/torch.py", line 559, in call_function
    tensor_variable = wrap_fx_proxy(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py", line 1302, in wrap_fx_proxy
    return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py", line 1387, in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 1590, in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 1545, in get_fake_value
    ret_val = wrap_fake_exception(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 1086, in wrap_fake_exception
    return fn()
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 1546, in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 1657, in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 1636, in run_node
    return node.target(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py", line 20, in wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1480, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py", line 1738, in dispatch
    r = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 571, in __call__
    return self_._op(*args, **(kwargs or {}))
  File "/usr/local/lib/python3.10/dist-packages/torch/_prims_common/wrappers.py", line 250, in _fn
    result = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_prims_common/wrappers.py", line 137, in _fn
    result = fn(**bound.arguments)
  File "/usr/local/lib/python3.10/dist-packages/torch/_refs/__init__.py", line 1033, in _ref
    a, b = _maybe_broadcast(a, b)
  File "/usr/local/lib/python3.10/dist-packages/torch/_refs/__init__.py", line 413, in _maybe_broadcast
    common_shape = _broadcast_shapes(
  File "/usr/local/lib/python3.10/dist-packages/torch/_refs/__init__.py", line 402, in _broadcast_shapes
    raise RuntimeError(
torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in method max of type object at 0x2ad16925ab60>(*(FakeTensor(..., device='cuda:0', size=(1, 8, 144), dtype=torch.float64), FakeTensor(..., device='cuda:0', size=(1, 8, 160))), **{}):
Attempting to broadcast a dimension of length 160 at -1! Mismatching argument at index 1 had torch.Size([1, 8, 160]); but expected shape should be broadcastable to [1, 8, 144]
from user code:
   File "/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py", line 486, in flash_attn_fwd_softmax_lse_correction
    max_scale = torch.max(softmax_lse, softmax_lse_per_step)
Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True