
=============
== PyTorch ==
=============

NVIDIA Release 24.02 (build 82611821)
PyTorch Version 2.3.0a0+ebedce2
Container image Copyright (c) 2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
Copyright (c) 2014-2024 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

ERROR: This container was built for NVIDIA Driver Release 545.23 or later, but
       version 470.161.03 was detected and compatibility mode is UNAVAILABLE.

       [[]]

[NeMo I 2024-04-23 09:19:04 train_gpt_sft:119] 
    
    ************** Experiment configuration ***********
[NeMo I 2024-04-23 09:19:04 train_gpt_sft:120] 
    name: megatron_gpt_sft
    trainer:
      num_nodes: 16
      devices: -1
      accelerator: gpu
      precision: 16-mixed
      sft:
        max_epochs: 1
        max_steps: -1
        val_check_interval: 12
        save_interval: 12
        limit_val_batches: 1
        gradient_clip_val: 1.0
      logger: false
      enable_checkpointing: false
      use_distributed_sampler: false
      max_time: null
      max_epochs: ${.sft.max_epochs}
      max_steps: ${.sft.max_steps}
    exp_manager:
      explicit_log_dir: /home/u9824269/LLM/llama3/train/result_dir
      exp_dir: null
      name: ${name}
      create_wandb_logger: true
      wandb_logger_kwargs:
        project: llama3-cp-debug
        name: dolly_sft_run_tp8_no_cp
      resume_if_exists: true
      resume_ignore_no_checkpoint: true
      create_checkpoint_callback: true
      checkpoint_callback_params:
        monitor: validation_loss
        save_top_k: 5
        mode: min
        save_nemo_on_train_end: true
        filename: megatron_gpt_sft--{${.monitor}:.3f}-{step}-{consumed_samples}-{epoch}
        model_parallel_size: ${model.tensor_model_parallel_size}
        save_best_model: false
    model:
      seed: 1234
      tensor_model_parallel_size: 8
      pipeline_model_parallel_size: 4
      restore_from_path: /home/u9824269/LLM/llama3/train/llama3-70b-16-mixed.nemo
      resume_from_checkpoint: null
      save_nemo_on_validation_end: true
      sync_batch_comm: false
      megatron_amp_O2: true
      encoder_seq_length: 4096
      sequence_parallel: true
      activations_checkpoint_granularity: selective
      activations_checkpoint_method: uniform
      activations_checkpoint_num_layers: null
      activations_checkpoint_layers_per_pipeline: null
      answer_only_loss: true
      gradient_as_bucket_view: false
      seq_len_interpolation_factor: null
      use_flash_attention: null
      hidden_dropout: 0.0
      attention_dropout: 0.0
      ffn_dropout: 0.0
      peft:
        peft_scheme: none
        restore_from_path: null
        lora_tuning:
          target_modules:
          - attention_qkv
          adapter_dim: 32
          adapter_dropout: 0.0
          column_init_method: xavier
          row_init_method: zero
          layer_selection: null
          weight_tying: false
          position_embedding_strategy: null
      data:
        chat: false
        chat_prompt_tokens:
          system_turn_start: "\0"
          turn_start: "\x11"
          label_start: "\x12"
          end_of_turn: '
    
            '
          end_of_name: '
    
            '
        sample: false
        num_workers: 0
        dataloader_type: single
        train_ds:
          file_path: /home/u9824269/LLM/nemo/databricks-dolly-15k-output.jsonl
          global_batch_size: 8
          micro_batch_size: 1
          shuffle: true
          memmap_workers: null
          max_seq_length: 8192
          min_seq_length: 1
          drop_last: true
          label_key: output
          add_eos: true
          add_sep: false
          add_bos: true
          truncation_field: input
          index_mapping_dir: null
          prompt_template: '<|start_header_id|>user<|end_header_id|>
    
    
            {input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
    
    
            {output}'
          hf_dataset: false
          truncation_method: right
        validation_ds:
          file_path: /home/u9824269/LLM/nemo/databricks-dolly-15k-output.jsonl
          global_batch_size: 128
          micro_batch_size: 1
          shuffle: false
          memmap_workers: ${model.data.train_ds.memmap_workers}
          max_seq_length: 8192
          min_seq_length: 1
          drop_last: true
          label_key: ${model.data.train_ds.label_key}
          add_eos: true
          add_sep: ${model.data.train_ds.add_sep}
          add_bos: true
          truncation_field: ${model.data.train_ds.truncation_field}
          index_mapping_dir: null
          prompt_template: ${model.data.train_ds.prompt_template}
          hf_dataset: false
          truncation_method: right
          output_original_text: true
      optim:
        name: distributed_fused_adam
        lr: 5.0e-06
        weight_decay: 0.01
        betas:
        - 0.9
        - 0.98
        sched:
          name: CosineAnnealing
          warmup_steps: 10
          constant_steps: 1000
          min_lr: 9.0e-07
      context_parallel_size: 1
    
[NeMo I 2024-04-23 09:19:05 exp_manager:396] Experiments will be logged at /home/u9824269/LLM/llama3/train/result_dir
[NeMo I 2024-04-23 09:19:05 exp_manager:842] TensorboardLogger has been set up
[NeMo I 2024-04-23 09:19:08 exp_manager:857] WandBLogger has been set up
[NeMo I 2024-04-23 09:27:03 megatron_init:251] Rank 0 has data parallel group : [0, 8, 16, 24]
[NeMo I 2024-04-23 09:27:03 megatron_init:257] Rank 0 has combined group of data parallel and context parallel : [0, 8, 16, 24]
[NeMo I 2024-04-23 09:27:03 megatron_init:262] All data parallel group ranks with context parallel combined: [[0, 8, 16, 24], [1, 9, 17, 25], [2, 10, 18, 26], [3, 11, 19, 27], [4, 12, 20, 28], [5, 13, 21, 29], [6, 14, 22, 30], [7, 15, 23, 31], [32, 40, 48, 56], [33, 41, 49, 57], [34, 42, 50, 58], [35, 43, 51, 59], [36, 44, 52, 60], [37, 45, 53, 61], [38, 46, 54, 62], [39, 47, 55, 63], [64, 72, 80, 88], [65, 73, 81, 89], [66, 74, 82, 90], [67, 75, 83, 91], [68, 76, 84, 92], [69, 77, 85, 93], [70, 78, 86, 94], [71, 79, 87, 95], [96, 104, 112, 120], [97, 105, 113, 121], [98, 106, 114, 122], [99, 107, 115, 123], [100, 108, 116, 124], [101, 109, 117, 125], [102, 110, 118, 126], [103, 111, 119, 127]]
[NeMo I 2024-04-23 09:27:03 megatron_init:265] Ranks 0 has data parallel rank: 0
[NeMo I 2024-04-23 09:27:03 megatron_init:282] Rank 0 has context parallel group: [0]
[NeMo I 2024-04-23 09:27:03 megatron_init:285] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
[NeMo I 2024-04-23 09:27:03 megatron_init:286] Ranks 0 has context parallel rank: 0
[NeMo I 2024-04-23 09:27:03 megatron_init:297] Rank 0 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7, 32, 33, 34, 35, 36, 37, 38, 39, 64, 65, 66, 67, 68, 69, 70, 71, 96, 97, 98, 99, 100, 101, 102, 103]
[NeMo I 2024-04-23 09:27:03 megatron_init:298] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7, 32, 33, 34, 35, 36, 37, 38, 39, 64, 65, 66, 67, 68, 69, 70, 71, 96, 97, 98, 99, 100, 101, 102, 103], [8, 9, 10, 11, 12, 13, 14, 15, 40, 41, 42, 43, 44, 45, 46, 47, 72, 73, 74, 75, 76, 77, 78, 79, 104, 105, 106, 107, 108, 109, 110, 111], [16, 17, 18, 19, 20, 21, 22, 23, 48, 49, 50, 51, 52, 53, 54, 55, 80, 81, 82, 83, 84, 85, 86, 87, 112, 113, 114, 115, 116, 117, 118, 119], [24, 25, 26, 27, 28, 29, 30, 31, 56, 57, 58, 59, 60, 61, 62, 63, 88, 89, 90, 91, 92, 93, 94, 95, 120, 121, 122, 123, 124, 125, 126, 127]]
[NeMo I 2024-04-23 09:27:03 megatron_init:308] Rank 0 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2024-04-23 09:27:03 megatron_init:312] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31], [32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47], [48, 49, 50, 51, 52, 53, 54, 55], [56, 57, 58, 59, 60, 61, 62, 63], [64, 65, 66, 67, 68, 69, 70, 71], [72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87], [88, 89, 90, 91, 92, 93, 94, 95], [96, 97, 98, 99, 100, 101, 102, 103], [104, 105, 106, 107, 108, 109, 110, 111], [112, 113, 114, 115, 116, 117, 118, 119], [120, 121, 122, 123, 124, 125, 126, 127]]
[NeMo I 2024-04-23 09:27:03 megatron_init:313] Rank 0 has tensor model parallel rank: 0
[NeMo I 2024-04-23 09:27:03 megatron_init:342] Rank 0 has pipeline model parallel group: [0, 32, 64, 96]
[NeMo I 2024-04-23 09:27:03 megatron_init:354] Rank 0 has embedding group: [0, 96]
[NeMo I 2024-04-23 09:27:03 megatron_init:360] All pipeline model parallel group ranks: [[0, 32, 64, 96], [1, 33, 65, 97], [2, 34, 66, 98], [3, 35, 67, 99], [4, 36, 68, 100], [5, 37, 69, 101], [6, 38, 70, 102], [7, 39, 71, 103], [8, 40, 72, 104], [9, 41, 73, 105], [10, 42, 74, 106], [11, 43, 75, 107], [12, 44, 76, 108], [13, 45, 77, 109], [14, 46, 78, 110], [15, 47, 79, 111], [16, 48, 80, 112], [17, 49, 81, 113], [18, 50, 82, 114], [19, 51, 83, 115], [20, 52, 84, 116], [21, 53, 85, 117], [22, 54, 86, 118], [23, 55, 87, 119], [24, 56, 88, 120], [25, 57, 89, 121], [26, 58, 90, 122], [27, 59, 91, 123], [28, 60, 92, 124], [29, 61, 93, 125], [30, 62, 94, 126], [31, 63, 95, 127]]
[NeMo I 2024-04-23 09:27:03 megatron_init:361] Rank 0 has pipeline model parallel rank 0
[NeMo I 2024-04-23 09:27:03 megatron_init:362] All embedding group ranks: [[0, 32, 64, 96], [1, 33, 65, 97], [2, 34, 66, 98], [3, 35, 67, 99], [4, 36, 68, 100], [5, 37, 69, 101], [6, 38, 70, 102], [7, 39, 71, 103], [8, 40, 72, 104], [9, 41, 73, 105], [10, 42, 74, 106], [11, 43, 75, 107], [12, 44, 76, 108], [13, 45, 77, 109], [14, 46, 78, 110], [15, 47, 79, 111], [16, 48, 80, 112], [17, 49, 81, 113], [18, 50, 82, 114], [19, 51, 83, 115], [20, 52, 84, 116], [21, 53, 85, 117], [22, 54, 86, 118], [23, 55, 87, 119], [24, 56, 88, 120], [25, 57, 89, 121], [26, 58, 90, 122], [27, 59, 91, 123], [28, 60, 92, 124], [29, 61, 93, 125], [30, 62, 94, 126], [31, 63, 95, 127]]
[NeMo I 2024-04-23 09:27:03 megatron_init:363] Rank 0 has embedding rank: 0
[NeMo I 2024-04-23 09:27:03 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /work/u4005115/models/llama/Meta-Llama-3-70B-Instruct/
[NeMo I 2024-04-23 09:27:04 megatron_base_model:544] Padded vocab_size: 129024, original vocab_size: 128256, dummy tokens: 768.
gn1101:28721:28721 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gn1101:28721:28721 [0] NCCL INFO Bootstrap : Using ib0:10.100.11.1<0>
gn1101:28721:28721 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
gn1101:28721:28721 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
gn1101:28721:28721 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
gn1101:28721:28721 [0] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
gn1101:28721:28721 [0] NCCL INFO cudaDriverVersion 12030
NCCL version 2.19.4+cuda12.3
gn1101:28721:9039 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
gn1101:28721:9039 [0] NCCL INFO P2P plugin IBext
gn1101:28721:9039 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gn1101:28721:9039 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [RO]; OOB ib0:10.100.11.1<0>
gn1101:28721:9039 [0] NCCL INFO Using non-device net plugin version 0
gn1101:28721:9039 [0] NCCL INFO Using network IBext
gn1101:28721:9039 [0] NCCL INFO comm 0x562c0dc89100 rank 0 nranks 128 cudaDev 0 nvmlDev 0 busId 1b000 commId 0xc30115f52475687c - Init START
gn1101:28721:9039 [0] NCCL INFO NVLS multicast support is not available on dev 0
gn1101:28721:9039 [0] NCCL INFO Channel 00/02 :    0   3   2   4   5   6   7   1   8  11  10  12  13  14  15   9  16  19  18  20
gn1101:28721:9039 [0] NCCL INFO Channel 01/02 :    0   3   2   4   5   6   7   1   8  11  10  12  13  14  15   9  16  19  18  20
gn1101:28721:9039 [0] NCCL INFO Trees [0] 1/64/-1->0->-1 [1] 1/-1/-1->0->8
gn1101:28721:9039 [0] NCCL INFO P2P Chunksize set to 131072
gn1101:28721:9039 [0] NCCL INFO Channel 00/0 : 0[0] -> 3[3] via P2P/CUMEM
gn1101:28721:9039 [0] NCCL INFO Channel 01/0 : 0[0] -> 3[3] via P2P/CUMEM
gn1101:28721:9039 [0] NCCL INFO Channel 00/0 : 121[1] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:9039 [0] NCCL INFO Channel 01/0 : 121[1] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:9039 [0] NCCL INFO Connected all rings
gn1101:28721:9039 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
gn1101:28721:9039 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
gn1101:28721:9039 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
gn1101:28721:9039 [0] NCCL INFO Channel 00/0 : 64[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:9039 [0] NCCL INFO Channel 00/0 : 0[0] -> 64[0] [send] via NET/IBext/0/GDRDMA
gn1101:28721:9039 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:9039 [0] NCCL INFO Connected all trees
gn1101:28721:9039 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
gn1101:28721:9039 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gn1101:28721:9039 [0] NCCL INFO Channel 00/1 : 0[0] -> 4[4] via P2P/indirect/2[2]
gn1101:28721:9039 [0] NCCL INFO Channel 01/1 : 0[0] -> 4[4] via P2P/indirect/2[2]
gn1101:28721:9039 [0] NCCL INFO Channel 00/1 : 0[0] -> 5[5] via P2P/indirect/6[6]
gn1101:28721:9039 [0] NCCL INFO Channel 01/1 : 0[0] -> 5[5] via P2P/indirect/6[6]
gn1101:28721:9039 [0] NCCL INFO Channel 00/1 : 0[0] -> 7[7] via P2P/indirect/1[1]
gn1101:28721:9039 [0] NCCL INFO Channel 01/1 : 0[0] -> 7[7] via P2P/indirect/1[1]
gn1101:28721:9039 [0] NCCL INFO comm 0x562c0dc89100 rank 0 nranks 128 cudaDev 0 nvmlDev 0 busId 1b000 commId 0xc30115f52475687c - Init COMPLETE
Loading distributed checkpoint with TensorStoreLoadShardedStrategy
[NeMo I 2024-04-23 09:42:22 nlp_overrides:1110] Model GPTSFTModel was successfully restored from /home/u9824269/LLM/llama3/train/llama3-70b-16-mixed.nemo.
[NeMo I 2024-04-23 09:42:22 train_script_utils:169] Running full finetuning since no peft scheme is given.
      | Name  | Type          | Params
    ----------------------------------------
    0 | model | Float16Module | 2.3 B 
    ----------------------------------------
    2.3 B     Trainable params
    0         Non-trainable params
    2.3 B     Total params
    9,086.173 Total estimated model params size (MB)
[NeMo I 2024-04-23 09:42:22 text_memmap_dataset:116] Building data files
[NeMo I 2024-04-23 09:42:22 text_memmap_dataset:525] Processing 1 data files using 18 workers
[NeMo I 2024-04-23 09:42:23 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.707857
[NeMo I 2024-04-23 09:42:24 text_memmap_dataset:525] Processing 1 data files using 18 workers
[NeMo I 2024-04-23 09:42:24 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.707153
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:158] Loading data files
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:249] Loading /home/u9824269/LLM/nemo/databricks-dolly-15k-output.jsonl
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.032507
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:165] Computing global indices
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:116] Building data files
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:525] Processing 1 data files using 18 workers
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.692256
[NeMo I 2024-04-23 09:42:25 text_memmap_dataset:525] Processing 1 data files using 18 workers
[NeMo I 2024-04-23 09:42:26 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.634395
[NeMo I 2024-04-23 09:42:26 text_memmap_dataset:158] Loading data files
[NeMo I 2024-04-23 09:42:26 text_memmap_dataset:249] Loading /home/u9824269/LLM/nemo/databricks-dolly-15k-output.jsonl
[NeMo I 2024-04-23 09:42:26 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.002066
[NeMo I 2024-04-23 09:42:26 text_memmap_dataset:165] Computing global indices
[NeMo I 2024-04-23 09:42:26 builders:328] Building dataloader with consumed samples: 0
[NeMo I 2024-04-23 09:42:26 builders:328] Building dataloader with consumed samples: 0
gn1101:28721:15210 [0] NCCL INFO Using non-device net plugin version 0
gn1101:28721:15210 [0] NCCL INFO Using network IBext
gn1101:28721:15210 [0] NCCL INFO comm 0x562c0e8cecb0 rank 0 nranks 32 cudaDev 0 nvmlDev 0 busId 1b000 commId 0xb8f5d8458f29be25 - Init START
gn1101:28721:15210 [0] NCCL INFO NVLS multicast support is not available on dev 0
gn1101:28721:15210 [0] NCCL INFO Channel 00/02 :    0   3   2   4   5   6   7   1   8  11  10  12  13  14  15   9  16  19  18  20
gn1101:28721:15210 [0] NCCL INFO Channel 01/02 :    0   3   2   4   5   6   7   1   8  11  10  12  13  14  15   9  16  19  18  20
gn1101:28721:15210 [0] NCCL INFO Trees [0] 1/16/-1->0->-1 [1] 1/-1/-1->0->8
gn1101:28721:15210 [0] NCCL INFO P2P Chunksize set to 131072
gn1101:28721:15210 [0] NCCL INFO Channel 00/0 : 0[0] -> 3[3] via P2P/CUMEM
gn1101:28721:15210 [0] NCCL INFO Channel 01/0 : 0[0] -> 3[3] via P2P/CUMEM
gn1101:28721:15210 [0] NCCL INFO Channel 00/0 : 25[1] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:15210 [0] NCCL INFO Channel 01/0 : 25[1] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:15210 [0] NCCL INFO Connected all rings
gn1101:28721:15210 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
gn1101:28721:15210 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
gn1101:28721:15210 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
gn1101:28721:15210 [0] NCCL INFO Channel 00/0 : 16[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:15210 [0] NCCL INFO Channel 00/0 : 0[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
gn1101:28721:15210 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:15210 [0] NCCL INFO Connected all trees
gn1101:28721:15210 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
gn1101:28721:15210 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gn1101:28721:15210 [0] NCCL INFO Channel 00/1 : 0[0] -> 4[4] via P2P/indirect/2[2]
gn1101:28721:15210 [0] NCCL INFO Channel 01/1 : 0[0] -> 4[4] via P2P/indirect/2[2]
gn1101:28721:15210 [0] NCCL INFO Channel 00/1 : 0[0] -> 5[5] via P2P/indirect/6[6]
gn1101:28721:15210 [0] NCCL INFO Channel 01/1 : 0[0] -> 5[5] via P2P/indirect/6[6]
gn1101:28721:15210 [0] NCCL INFO Channel 00/1 : 0[0] -> 7[7] via P2P/indirect/1[1]
gn1101:28721:15210 [0] NCCL INFO Channel 01/1 : 0[0] -> 7[7] via P2P/indirect/1[1]
gn1101:28721:15210 [0] NCCL INFO comm 0x562c0e8cecb0 rank 0 nranks 32 cudaDev 0 nvmlDev 0 busId 1b000 commId 0xb8f5d8458f29be25 - Init COMPLETE
[NeMo I 2024-04-23 09:42:35 megatron_gpt_model:1306] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 2.27e+09. Total number of model parameters: 7.06e+10.
[NeMo I 2024-04-23 09:42:36 modelPT:723] Optimizer config = MegatronDistributedFusedAdam (
    Parameter Group 0
        betas: [0.9, 0.98]
        bias_correction: True
        eps: 1e-08
        lr: 5e-06
        weight_decay: 0.01
    
    Parameter Group 1
        betas: [0.9, 0.98]
        bias_correction: True
        eps: 1e-08
        lr: 5e-06
        weight_decay: 0.0
    )
[NeMo I 2024-04-23 09:42:36 lr_scheduler:915] Scheduler "<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x2b1c130bf3d0>" 
    will be used during training (effective maximum steps = 1876) - 
    Parameters : 
    (warmup_steps: 10
    constant_steps: 1000
    min_lr: 9.0e-07
    max_steps: 1876
    )
gn1101:28721:15718 [0] NCCL INFO Using non-device net plugin version 0
gn1101:28721:15718 [0] NCCL INFO Using network IBext
gn1101:28721:15718 [0] NCCL INFO comm 0x562c1a2e13b0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 1b000 commId 0xd2edcb9b0c48cb78 - Init START
gn1101:28721:15718 [0] NCCL INFO NVLS multicast support is not available on dev 0
gn1101:28721:15718 [0] NCCL INFO Channel 00/12 :    0   3   2   1   7   4   5   6
gn1101:28721:15718 [0] NCCL INFO Channel 01/12 :    0   3   2   1   7   4   5   6
gn1101:28721:15718 [0] NCCL INFO Channel 02/12 :    0   6   5   4   7   1   2   3
gn1101:28721:15718 [0] NCCL INFO Channel 03/12 :    0   6   5   4   7   1   2   3
gn1101:28721:15718 [0] NCCL INFO Channel 04/12 :    0   1   3   5   7   6   4   2
gn1101:28721:15718 [0] NCCL INFO Channel 05/12 :    0   2   4   6   7   5   3   1
gn1101:28721:15718 [0] NCCL INFO Channel 06/12 :    0   3   2   1   7   4   5   6
gn1101:28721:15718 [0] NCCL INFO Channel 07/12 :    0   3   2   1   7   4   5   6
gn1101:28721:15718 [0] NCCL INFO Channel 08/12 :    0   6   5   4   7   1   2   3
gn1101:28721:15718 [0] NCCL INFO Channel 09/12 :    0   6   5   4   7   1   2   3
gn1101:28721:15718 [0] NCCL INFO Channel 10/12 :    0   1   3   5   7   6   4   2
gn1101:28721:15718 [0] NCCL INFO Channel 11/12 :    0   2   4   6   7   5   3   1
gn1101:28721:15718 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1 [1] 3/-1/-1->0->-1 [2] 6/-1/-1->0->-1 [3] 6/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 3/-1/-1->0->-1 [7] 3/-1/-1->0->-1 [8] 6/-1/-1->0->-1 [9] 6/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 2/-1/-1->0->-1
gn1101:28721:15718 [0] NCCL INFO P2P Chunksize set to 524288
gn1101:28721:15718 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM
gn1101:28721:15718 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM
gn1101:28721:15718 [0] NCCL INFO Channel 05/0 : 0[0] -> 2[2] via P2P/CUMEM
gn1101:28721:15718 [0] NCCL INFO Channel 11/0 : 0[0] -> 2[2] via P2P/CUMEM
gn1101:28721:15718 [0] NCCL INFO Channel 00/0 : 0[0] -> 3[3] via P2P/CUMEM
gn1101:28721:15718 [0] NCCL INFO Channel 01/0 : 0[0] -> 3[3] via P2P/CUMEM
gn1101:28721:15718 [0] NCCL INFO Channel 06/0 : 0[0] -> 3[3] via P2P/CUMEM
gn1101:28721:15718 [0] NCCL INFO Channel 07/0 : 0[0] -> 3[3] via P2P/CUMEM
gn1101:28721:15718 [0] NCCL INFO Channel 02/0 : 0[0] -> 6[6] via P2P/CUMEM
gn1101:28721:15718 [0] NCCL INFO Channel 03/0 : 0[0] -> 6[6] via P2P/CUMEM
gn1101:28721:15718 [0] NCCL INFO Channel 08/0 : 0[0] -> 6[6] via P2P/CUMEM
gn1101:28721:15718 [0] NCCL INFO Channel 09/0 : 0[0] -> 6[6] via P2P/CUMEM
gn1101:28721:15718 [0] NCCL INFO Connected all rings
gn1101:28721:15718 [0] NCCL INFO Connected all trees
gn1101:28721:15718 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gn1101:28721:15718 [0] NCCL INFO 12 coll channels, 0 nvls channels, 16 p2p channels, 2 p2p channels per peer
gn1101:28721:15718 [0] NCCL INFO Channel 02/1 : 0[0] -> 4[4] via P2P/indirect/2[2]
gn1101:28721:15718 [0] NCCL INFO Channel 03/1 : 0[0] -> 4[4] via P2P/indirect/2[2]
gn1101:28721:15718 [0] NCCL INFO Channel 10/1 : 0[0] -> 5[5] via P2P/indirect/6[6]
gn1101:28721:15718 [0] NCCL INFO Channel 11/1 : 0[0] -> 5[5] via P2P/indirect/6[6]
gn1101:28721:15718 [0] NCCL INFO Channel 14/1 : 0[0] -> 7[7] via P2P/indirect/1[1]
gn1101:28721:15718 [0] NCCL INFO Channel 15/1 : 0[0] -> 7[7] via P2P/indirect/1[1]
gn1101:28721:15718 [0] NCCL INFO comm 0x562c1a2e13b0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 1b000 commId 0xd2edcb9b0c48cb78 - Init COMPLETE
gn1101:28721:16588 [0] NCCL INFO Using non-device net plugin version 0
gn1101:28721:16588 [0] NCCL INFO Using network IBext
gn1101:28721:16588 [0] NCCL INFO comm 0x562c2006afd0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1b000 commId 0x2b8ecc8e410bfe8 - Init START
gn1101:28721:16588 [0] NCCL INFO Channel 00/02 :    0   1   2   3
gn1101:28721:16588 [0] NCCL INFO Channel 01/02 :    0   1   2   3
gn1101:28721:16588 [0] NCCL INFO Trees [0] 2/-1/-1->0->-1 [1] -1/-1/-1->0->1
gn1101:28721:16588 [0] NCCL INFO P2P Chunksize set to 131072
gn1101:28721:16588 [0] NCCL INFO Channel 00/0 : 3[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:16588 [0] NCCL INFO Channel 01/0 : 3[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:16588 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IBext/0/GDRDMA
gn1101:28721:16588 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IBext/0/GDRDMA
gn1101:28721:16588 [0] NCCL INFO Connected all rings
gn1101:28721:16588 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:16588 [0] NCCL INFO Channel 00/0 : 0[0] -> 2[0] [send] via NET/IBext/0/GDRDMA
gn1101:28721:16588 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:16588 [0] NCCL INFO Connected all trees
gn1101:28721:16588 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gn1101:28721:16588 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gn1101:28721:16588 [0] NCCL INFO comm 0x562c2006afd0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1b000 commId 0x2b8ecc8e410bfe8 - Init COMPLETE
gn1101:28721:16637 [0] NCCL INFO Channel 00/1 : 0[0] -> 1[0] [send] via NET/IBext/0/GDRDMA/Shared
gn1101:28721:16637 [0] NCCL INFO Channel 01/1 : 0[0] -> 1[0] [send] via NET/IBext/0/GDRDMA/Shared
gn1101:28721:16732 [0] NCCL INFO Channel 00/1 : 1[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA/Shared
gn1101:28721:16732 [0] NCCL INFO Channel 01/1 : 1[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA/Shared
gn1101:28721:20270 [0] NCCL INFO Using non-device net plugin version 0
gn1101:28721:20270 [0] NCCL INFO Using network IBext
gn1101:28721:20270 [0] NCCL INFO comm 0x2b220207c8d0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1b000 commId 0x50361a548046e9e5 - Init START
gn1101:28721:20270 [0] NCCL INFO Channel 00/02 :    0   1   2   3
gn1101:28721:20270 [0] NCCL INFO Channel 01/02 :    0   1   2   3
gn1101:28721:20270 [0] NCCL INFO Trees [0] 2/-1/-1->0->-1 [1] -1/-1/-1->0->1
gn1101:28721:20270 [0] NCCL INFO P2P Chunksize set to 131072
gn1101:28721:20270 [0] NCCL INFO Channel 00/0 : 3[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:20270 [0] NCCL INFO Channel 01/0 : 3[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:20270 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IBext/0/GDRDMA
gn1101:28721:20270 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IBext/0/GDRDMA
gn1101:28721:20270 [0] NCCL INFO Connected all rings
gn1101:28721:20270 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:20270 [0] NCCL INFO Channel 00/0 : 0[0] -> 2[0] [send] via NET/IBext/0/GDRDMA
gn1101:28721:20270 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
gn1101:28721:20270 [0] NCCL INFO Connected all trees
gn1101:28721:20270 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gn1101:28721:20270 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gn1101:28721:20270 [0] NCCL INFO comm 0x2b220207c8d0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1b000 commId 0x50361a548046e9e5 - Init COMPLETE
